{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Definition (Wasserstein Distance)**\n",
        "\n",
        "Let $P$ and $Q$ be two probability distributions and denote by $\\Pi(P,Q)$ the set of probability distributions whose marginals are $P$ and $Q.$ We call Wasserstein distance between $P$ and $Q$ the distance\n",
        "$$\n",
        "W(P,Q)=\\inf_{\\gamma\\in\\Pi(P,Q)}\\mathbb{E}_{(x,y)\\sim \\gamma}\\|x-y\\|.\n",
        "$$\n",
        "Intuitively, $W(P,Q)$ measures the minimal effort required to move mass around to change $P$ into $Q,$ or $Q$ into $P.$\n",
        "\n",
        "**Theorem (Kantorovich-Rubinstein Duality)**\n",
        "\n",
        "For all probability distributions $P$ and $Q,$ we have\n",
        "$$\n",
        "W(P,Q)=\\sup_{{\\|f\\|\\ }_L\\leqslant 1} \\mathbb{E}_{x\\sim P}[f(x)]-\\mathbb{E}_{x\\sim Q}[f(x)]\n",
        "$$\n",
        "where $\\|\\cdot\\|_L$ denotes the Lipschitz seminorm.\n",
        "\n",
        "**Notations**\n",
        "\n",
        "- We shall denote by $P_r$ the real distribution of the data.\n",
        "\n",
        "- We shall denote by $\\theta$ the weights of the generator $g_\\theta$ and by $P_\\theta$ the probability distribution of $g_\\theta.$ We shall assume $P_\\theta=g_\\theta(Z)$ where $Z$ is a fixed distribution on a low-dimensional space.\n",
        "\n",
        "- We shall denote by $w$ the weights of the discriminator $f_w,$ called the critic in the WGAN framework. If $x$ is a data sample, then $f_w(x)$ is the score given to $x$ by $f_w.$ In the WGAN model, the score is not binary, but real-valued.\n",
        "\n",
        "**Problem Formulation**\n",
        "\n",
        "- Our goal is to choose $\\theta$ to approximate $\\inf_{\\theta}W(P_r,P_\\theta),$ which, by the **Kantorovich-Rubinstein duality theorem**, is equivalent to approximating\n",
        "$$\n",
        "\\inf_{\\theta}\\sup_{{\\|f\\|\\ }_L\\leqslant 1} \\mathbb{E}_{x\\sim P_r}[f(x)]-\\mathbb{E}_{z\\sim Z}[f\\circ g_\\theta(z)].\n",
        "$$\n",
        "\n",
        "- We thus also need to choose an adequate $f.$ This is the role of the critic $f_w$ with weights $w.$ Indeed, up to a multiplicative constant, we can use the approximation\n",
        "$$\n",
        "\\sup_{{\\|f\\|\\ }_L\\leqslant 1} \\mathbb{E}_{x\\sim P_r}[f(x)]-\\mathbb{E}_{z\\sim Z}[f\\circ g_\\theta(z)]\\approx \\sup_{-c\\leqslant w\\leqslant c} \\mathbb{E}_{x\\sim P_r}[f_w(x)]-\\mathbb{E}_{z\\sim Z}[f_w\\circ g_\\theta(z)]\n",
        "$$\n",
        "where $c$ is a constant such that $-c\\leqslant w\\leqslant c$ means that all the weights in $w$ are between $-c$ and $c,$ simulating a Lipschtiz constant.\n",
        "\n",
        "- Hence our problem becomes finding $\\theta$ and $w$ that approximate\n",
        "$$\n",
        "\\inf_{\\theta}\\sup_{-c\\leqslant w\\leqslant c} \\mathbb{E}_{x\\sim P_r}[f_w(x)]-\\mathbb{E}_{z\\sim Z}[f_w\\circ g_\\theta(z)]=\\inf_{\\theta}\\inf_{-c\\leqslant w\\leqslant c} \\mathbb{E}_{z\\sim Z}[f_w\\circ g_\\theta(z)]-\\mathbb{E}_{x\\sim P_r}[f_w(x)].\n",
        "$$\n",
        "\n",
        "- In practice, we shall independantly sample $x_1,\\ldots,x_n\\sim P_r$ and $z_1,\\ldots,z_m\\sim Z$ and find $\\theta$ and $w$ that approximate\n",
        "$$\n",
        "\\boxed{\\inf_{\\theta}\\inf_{-c\\leqslant w\\leqslant c} \\sum_{i=1}^mf_w\\circ g_\\theta(z_i)-\\sum_{i=1}^nf_w(x_i).}\n",
        "$$\n",
        "\n",
        "**Critic Loss**\n",
        "\n",
        "Given a fixed $\\theta,$ the loss function for the critic is\n",
        "$$\n",
        "\\boxed{L_\\text{disc}(w)=\\sum_{i=1}^mf_w\\circ g_\\theta(z_i)-\\sum_{i=1}^nf_w(x_i).}\n",
        "$$\n",
        "In other words, we have\n",
        "$$\n",
        "\\boxed{L_\\text{disc}(w)=\\text{mean}(\\text{fake data scores})-\\text{mean}(\\text{real data scores}).}\n",
        "$$\n",
        "\n",
        "**Generator Loss**\n",
        "\n",
        "- Assume $w$ is fixed and that $W(P_r,P_\\theta)$ is approximated by\n",
        "$$\n",
        "W(P_r,P_\\theta)\\approx \\mathbb{E}_{x\\sim P_r}[f_w(x)]-\\mathbb{E}_{z\\sim Z}[f_w\\circ g_\\theta(z)].\n",
        "$$\n",
        "\n",
        "- We are thus looking to approximate\n",
        "$$\n",
        "\\underset{\\theta}{\\text{arginf}}\\ W(P_r,P_\\theta)\\approx \\underset{\\theta}{\\text{arginf}}\\ \\mathbb{E}_{x\\sim P_r}[f_w(x)]-\\mathbb{E}_{z\\sim Z}[f_w\\circ g_\\theta(z)].\n",
        "$$\n",
        "\n",
        "- Because $\\mathbb{E}_{x\\sim P_r}[f_w(x)]$ is independant of $\\theta,$ the problem is equivalent to solving\n",
        "$$\n",
        "\\underset{\\theta}{\\text{arginf}}\\ -\\mathbb{E}_{z\\sim Z}[f_w\\circ g_\\theta(z)].\n",
        "$$\n",
        "\n",
        "- In practice, we independantly sample $z_1,\\ldots,z_m\\sim Z$ from the real data and solve\n",
        "$$\n",
        "\\underset{\\theta}{\\text{arginf}}\\ -\\sum_{i=1}^mf_w\\circ g_\\theta(z_i).\n",
        "$$\n",
        "\n",
        "- Hence the loss function for the generator is\n",
        "$$\n",
        "\\boxed{L_\\text{gen}(\\theta)=-\\sum_{i=1}^mf_w\\circ g_\\theta(z_i).}\n",
        "$$\n",
        "In other words, we have\n",
        "$$\n",
        "\\boxed{L_\\text{gen}(\\theta)=-\\text{mean}(\\text{fake data scores}).}\n",
        "$$"
      ],
      "metadata": {
        "id": "AGyqHEgjRMPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable"
      ],
      "metadata": {
        "id": "uDWpl-x-kXi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "bMF_6RFznVMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generator class\n",
        "class Generator(nn.Module):\n",
        "\n",
        "    # Initialize network\n",
        "    def __init__(self, g_output_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.fc1 = nn.Linear(100, 256)\n",
        "        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features * 2)\n",
        "        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features * 2)\n",
        "        self.fc4 = nn.Linear(self.fc3.out_features, g_output_dim)\n",
        "\n",
        "    # Feed forward\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
        "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "        return torch.tanh(self.fc4(x))"
      ],
      "metadata": {
        "id": "fH2Hg3msniJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Critic class\n",
        "class Critic(nn.Module):\n",
        "\n",
        "    # Initialize network\n",
        "    def __init__(self, d_input_dim):\n",
        "        super(Critic, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_input_dim, 1024)\n",
        "        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features // 2)\n",
        "        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features // 2)\n",
        "        self.fc4 = nn.Linear(self.fc3.out_features, 1)\n",
        "\n",
        "    # Feed forward\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
        "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "        return self.fc4(x)  # No sigmoid, WGAN outputs raw scores"
      ],
      "metadata": {
        "id": "kQeRTbrqoJ1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Critic loss function\n",
        "def critic_loss(real_data_score, fake_data_score):\n",
        "    return torch.mean(fake_data_score) - torch.mean(real_data_score)\n",
        "\n",
        "# Generator loss function\n",
        "def generator_loss(fake_data_score):\n",
        "    return -torch.mean(fake_data_score)"
      ],
      "metadata": {
        "id": "6_9yK99MozFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Weight clipping function\n",
        "def clip_weights(model, clip_value):\n",
        "    for param in model.parameters():\n",
        "        param.data.clamp_(-clip_value, clip_value)"
      ],
      "metadata": {
        "id": "UOyGYl82pfli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WGAN model training\n",
        "def train_wgan(critic, generator, real_data_loader, num_epochs, n_critic = 5, lr = 0.00005, clip_value = 0.01):\n",
        "\n",
        "    # Initialize optimizers\n",
        "    optimizer_c = optim.RMSprop(critic.parameters(), lr = lr)  # Critic optimizer\n",
        "    optimizer_g = optim.RMSprop(generator.parameters(), lr = lr)  # Generator optimizer\n",
        "\n",
        "    # Move models to device\n",
        "    critic.to(device)\n",
        "    generator.to(device)\n",
        "\n",
        "    # Loop over number of epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        for real_data in real_data_loader:  # Loop over each batch of real data\n",
        "            real_data = real_data.to(device)  # Move real data to device\n",
        "            batch_size = real_data.size(0)  # Get size of current batch\n",
        "\n",
        "            # Train critic n_critic times\n",
        "            for _ in range(n_critic):\n",
        "\n",
        "                # Generate fake data\n",
        "                noise = torch.randn(batch_size, 100, device = device)  # Create random noise\n",
        "                fake_data = generator(noise).detach()  # Output fake data from generator\n",
        "\n",
        "                # Compute critic loss\n",
        "                optimizer_c.zero_grad()  # Clear previous gradients for critic\n",
        "                loss_c = critic_loss(critic(real_data), critic(fake_data))  # Calculate critic loss\n",
        "                loss_c.backward()  # Backpropagate loss\n",
        "                optimizer_c.step()  # Update critic weights\n",
        "\n",
        "                # Clip critic weights\n",
        "                clip_weights(critic, clip_value)\n",
        "\n",
        "            # Train generator\n",
        "            noise = torch.randn(batch_size, 100, device = device)  # Create random noise\n",
        "            fake_data = generator(noise)  # Output fake data from generator\n",
        "\n",
        "            optimizer_g.zero_grad()  # Clear previous gradients for generator\n",
        "            loss_g = generator_loss(critic(fake_data))  # Calculate generator loss\n",
        "            loss_g.backward()  # Backpropagate loss\n",
        "            optimizer_g.step()  # Update generator weights\n",
        "\n",
        "        # Print loss for each epoch to track training progress\n",
        "        print(f\"Epoch [{epoch} / {num_epochs}], Critic Loss: {loss_c.item()}, Generator Loss: {loss_g.item()}\")"
      ],
      "metadata": {
        "id": "AUUHrEVMsJqT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}